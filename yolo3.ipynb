{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"yolo3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOOrWZ474440wJrbUKfeSJJ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"yUC8mE3PO8dZ","executionInfo":{"status":"error","timestamp":1600063960440,"user_tz":-480,"elapsed":640,"user":{"displayName":"黃鈺倫 Intern","photoUrl":"","userId":"07415287677551779104"}},"outputId":"3461fe66-b527-4044-a44f-74674c5dc31c","colab":{"base_uri":"https://localhost:8080/","height":197}},"source":["img = [10,20,[10,20,255]]\n","img_ = img[:,:,::-1]\n","img_"],"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-350c632c6ac3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimg_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"]}]},{"cell_type":"code","metadata":{"id":"eZ67U_aVN2Ym"},"source":["from __future__ import division\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import numpy as np\n","import cv2\n","\n","def get_test_input():\n","  img = cv2.imread(\"dog-cycle-car.png\")\n","  img = cv2.resize(img, (416,416))\n","  img_ = img[:,:,::-1].transpose((2,0,1)) \n","  #img是【h,w,channel】，这里的img[:,:,::-1]是将第三个维度channel从opencv的BGR转化为pytorch的RGB，\n","  # 然后transpose((2,0,1))的意思是将[height,width,channel]->[channel,height,width]\n","  # ::-1 means \"take everything in this dimension but backwards\n","  img_ = img_[np.newaxis,:,:,:]/255  #Add a channel at 0 (for batch) |then Normalise\n","  img_ = torch.from_numpy(img_).float()\n","  img_ = Variable(img_)\n","  return img_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hHjos7sjP6E4"},"source":["處理圖片注意事項：\n","1. 用甚麼讀的，讀進來是h,w,c還是c,h,w(pytorch用)\n","2. channel裡面是RGB還是BGR\n","3. 新增sample數量/batch的維度\n","4. normalize(divided by 255)\n","5. 轉成pytorch tensor\n"]},{"cell_type":"code","metadata":{"id":"AYRoJVIxQ7GU"},"source":["def parse_cfg(cfgfile):\n","    \"\"\"\n","    输入: 配置文件路径\n","    返回值: 列表对象,其中每一个元素为一个字典类型对应于一个要建立的神经网络模块（层）\n","    \n","    \"\"\"\n","    # 加载文件并过滤掉文本中多余内容\n","    file = open(cfgfile, 'r')\n","    lines = file.read().split('\\n')                        # store the lines in a list等价于readlines\n","    lines = [x for x in lines if len(x) > 0]               # 去掉空行\n","    lines = [x for x in lines if x[0] != '#']              # 去掉以#开头的注释行\n","    lines = [x.rstrip().lstrip() for x in lines]           # 去掉左右两边的空格(rstricp是去掉右边的空格，lstrip是去掉左边的空格)\n","    # cfg文件中的每个块用[]括起来最后组成一个列表，一个block存储一个块的内容，即每个层用一个字典block存储。\n","    block = {}\n","    blocks = []\n","    \n","    for line in lines:\n","        if line[0] == \"[\":               # 这是cfg文件中一个层(块)的开始           \n","            if len(block) != 0:          # 如果块内已经存了信息, 说明是上一个块的信息还没有保存\n","                blocks.append(block)     # 那么这个块（字典）加入到blocks列表中去\n","                block = {}               # 覆盖掉已存储的block,新建一个空白块存储描述下一个块的信息(block是字典)\n","            block[\"type\"] = line[1:-1].rstrip()  # 把cfg的[]中的块名作为键type的值   \n","        else:\n","            key,value = line.split(\"=\") #按等号分割\n","            block[key.rstrip()] = value.lstrip()#左边是key(去掉右空格)，右边是value(去掉左空格)，形成一个block字典的键值对\n","    blocks.append(block) # 退出循环，将最后一个未加入的block加进去\n","    # print('\\n\\n'.join([repr(x) for x in blocks]))\n","    return blocks\n","\n","# 配置文件定义了6种不同type\n","# 'net': 相当于超参数,网络全局配置的相关参数\n","# {'convolutional', 'net', 'route', 'shortcut', 'upsample', 'yolo'}\n","\n","# cfg = parse_cfg(\"cfg/yolov3.cfg\")\n","# print(cfg)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pS6HlxEpTDPL"},"source":["class EmptyLayer(nn.Module):\n","    \"\"\"\n","    为shortcut layer / route layer 准备, 具体功能不在此实现，在Darknet类的forward函数中有体现\n","    \"\"\"\n","    def __init__(self):\n","        super(EmptyLayer, self).__init__()\n","        \n","\n","class DetectionLayer(nn.Module):\n","    '''yolo 检测层的具体实现, 在特征图上使用锚点预测目标区域和类别, 功能函数在predict_transform中'''\n","    def __init__(self, anchors):\n","        super(DetectionLayer, self).__init__()\n","        self.anchors = anchors\n","\n","\n","\n","def create_modules(blocks):\n","    net_info = blocks[0]     # blocks[0]存储了cfg中[net]的信息，它是一个字典，获取网络输入和预处理相关信息    \n","    module_list = nn.ModuleList() # module_list用于存储每个block,每个block对应cfg文件中一个块，类似[convolutional]里面就对应一个卷积块\n","    prev_filters = 3   #初始值对应于输入数据3通道，用来存储我们需要持续追踪被应用卷积层的卷积核数量（上一层的卷积核数量（或特征图深度））\n","    output_filters = []   #我们不仅需要追踪前一层的卷积核数量，还需要追踪之前每个层。随着不断地迭代，我们将每个模块的输出卷积核数量添加到 output_filters 列表上。\n","    \n","    for index, x in enumerate(blocks[1:]): #这里，我们迭代block[1:] 而不是blocks，因为blocks的第一个元素是一个net块，它不属于前向传播。\n","        module = nn.Sequential()# 这里每个块用nn.sequential()创建为了一个module,一个module有多个层\n","    \n","        #check the type of block\n","        #create a new module for the block\n","        #append to module_list\n","        \n","        if (x[\"type\"] == \"convolutional\"):\n","            ''' 1. 卷积层 '''\n","            # 获取激活函数/批归一化/卷积层参数（通过字典的键获取值）\n","            activation = x[\"activation\"]\n","            try:\n","                batch_normalize = int(x[\"batch_normalize\"])\n","                bias = False#卷积层后接BN就不需要bias\n","            except:\n","                batch_normalize = 0\n","                bias = True #卷积层后无BN层就需要bias\n","        \n","            filters= int(x[\"filters\"])\n","            padding = int(x[\"pad\"])\n","            kernel_size = int(x[\"size\"])\n","            stride = int(x[\"stride\"])\n","        \n","            if padding:\n","                pad = (kernel_size - 1) // 2\n","            else:\n","                pad = 0\n","        \n","            # 开始创建并添加相应层\n","            # Add the convolutional layer\n","            # nn.Conv2d(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True)\n","            conv = nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias = bias)\n","            module.add_module(\"conv_{0}\".format(index), conv)\n","        \n","            #Add the Batch Norm Layer\n","            if batch_normalize:\n","                bn = nn.BatchNorm2d(filters)\n","                module.add_module(\"batch_norm_{0}\".format(index), bn)\n","        \n","            #Check the activation. \n","            #It is either Linear or a Leaky ReLU for YOLO\n","            # 给定参数负轴系数0.1\n","            if activation == \"leaky\":\n","                activn = nn.LeakyReLU(0.1, inplace = True)\n","                module.add_module(\"leaky_{0}\".format(index), activn)\n","                   \n","        elif (x[\"type\"] == \"upsample\"):\n","            '''\n","            2. upsampling layer\n","            没有使用 Bilinear2dUpsampling\n","            实际使用的为最近邻插值\n","            '''\n","            stride = int(x[\"stride\"])#这个stride在cfg中就是2，所以下面的scale_factor写2或者stride是等价的\n","            upsample = nn.Upsample(scale_factor = 2, mode = \"nearest\")\n","            module.add_module(\"upsample_{}\".format(index), upsample)\n","                \n","        # route layer -> Empty layer\n","        # route层的作用：当layer取值为正时，输出这个正数对应的层的特征，如果layer取值为负数，输出route层向后退layer层对应层的特征\n","        elif (x[\"type\"] == \"route\"):\n","            x[\"layers\"] = x[\"layers\"].split(',')\n","            #Start  of a route\n","            start = int(x[\"layers\"][0])\n","            #end, if there exists one.\n","            try:\n","                end = int(x[\"layers\"][1])\n","            except:\n","                end = 0\n","            #Positive anotation: 正值\n","            if start > 0: \n","                start = start - index            \n","            if end > 0:# 若end>0，由于end= end - index，再执行index + end输出的还是第end层的特征\n","                end = end - index\n","            route = EmptyLayer()\n","            module.add_module(\"route_{0}\".format(index), route)\n","            if end < 0: #若end<0，则end还是end，输出index+end(而end<0)故index向后退end层的特征。\n","                filters = output_filters[index + start] + output_filters[index + end]\n","            else: #如果没有第二个参数，end=0，则对应下面的公式，此时若start>0，由于start = start - index，再执行index + start输出的还是第start层的特征;若start<0，则start还是start，输出index+start(而start<0)故index向后退start层的特征。\n","                filters= output_filters[index + start]\n","    \n","        #shortcut corresponds to skip connection\n","        elif x[\"type\"] == \"shortcut\":\n","            shortcut = EmptyLayer() #使用空的层，因为它还要执行一个非常简单的操作（加）。没必要更新 filters 变量,因为它只是将前一层的特征图添加到后面的层上而已。\n","            module.add_module(\"shortcut_{}\".format(index), shortcut)\n","            \n","        #Yolo is the detection layer\n","        elif x[\"type\"] == \"yolo\":\n","            mask = x[\"mask\"].split(\",\")\n","            mask = [int(x) for x in mask]\n","    \n","            anchors = x[\"anchors\"].split(\",\")\n","            anchors = [int(a) for a in anchors]\n","            anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)]\n","            anchors = [anchors[i] for i in mask]\n","    \n","            detection = DetectionLayer(anchors)# 锚点,检测,位置回归,分类，这个类见predict_transform中\n","            module.add_module(\"Detection_{}\".format(index), detection)\n","                              \n","        module_list.append(module)\n","        prev_filters = filters\n","        output_filters.append(filters)\n","        \n","    return (net_info, module_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qTNDkCRrVsVO"},"source":["筆記\n","1. #卷积层后无BN层就需要bias???\n","2. leakyReLU的參數、inplace?\n","3. over/undersampling,  up/downsampling\n","4. route裡面的filters為何要相加？"]},{"cell_type":"code","metadata":{"id":"rgufIBuopgtU"},"source":["\n","class Darknet(nn.Module):\n","    def __init__(self, cfgfile):\n","        super(Darknet, self).__init__()\n","        self.blocks = parse_cfg(cfgfile) #调用parse_cfg函数\n","        self.net_info, self.module_list = create_modules(self.blocks)#调用create_modules函数\n","        \n","    def forward(self, x, CUDA):\n","        modules = self.blocks[1:] # 除了net块之外的所有，forward这里用的是blocks列表中的各个block块字典\n","        outputs = {}   #We cache the outputs for the route layer，待會會放 \"層數\":\"特徵\"\n","        \n","        write = 0#write表示我们是否遇到第一个检测。write=0，则收集器尚未初始化，write=1，则收集器已经初始化，我们只需要将检测图与收集器级联起来即可。\n","        for i, module in enumerate(modules):      #iter cfg檔中的文字blocks  \n","            module_type = (module[\"type\"])\n","            \n","            if module_type == \"convolutional\" or module_type == \"upsample\":\n","                x = self.module_list[i](x) #傳遞特徵到層裡面\n","    \n","            elif module_type == \"route\":\n","                layers = module[\"layers\"]\n","                layers = [int(a) for a in layers]\n","    \n","                if (layers[0]) > 0:\n","                    layers[0] = layers[0] - i\n","                # 如果只有一层时。从前面的if (layers[0]) > 0:语句中可知，如果layer[0]>0，则输出的就是当前layer[0]这一层的特征,如果layer[0]<0，输出就是从route层(第i层)向后退layer[0]层那一层得到的特征 \n","                if len(layers) == 1:\n","                    x = outputs[i + (layers[0])]\n","                #第二个元素同理 \n","                else:\n","                    if (layers[1]) > 0:\n","                        layers[1] = layers[1] - i #若layer[1] = 61，代表要跳到第61層(第61個block) ，則先減掉現在在的層數，再於下方加總\n","    \n","                    map1 = outputs[i + layers[0]]\n","                    map2 = outputs[i + layers[1]]\n","                    x = torch.cat((map1, map2), 1)#第二个参数设为 1,这是因为我们希望将特征图沿anchor数量的维度级联起来。\n","                \n","    \n","            elif  module_type == \"shortcut\":\n","                from_ = int(module[\"from\"])\n","                x = outputs[i-1] + outputs[i+from_] # 求和运算，它只是将前一层的特征图添加到后面的层上而已\n","            \n","            elif module_type == 'yolo':        \n","                anchors = self.module_list[i][0].anchors\n","                #从net_info(实际就是blocks[0]，即[net])中get the input dimensions\n","                inp_dim = int (self.net_info[\"height\"])\n","        \n","                #Get the number of classes\n","                num_classes = int (module[\"classes\"])\n","        \n","                #Transform \n","                x = x.data # 这里得到的是预测的yolo层feature map\n","                # 在util.py中的predict_transform()函数利用x(是传入yolo层的feature map)，得到每个格子所对应的anchor最终得到的目标\n","                # 坐标与宽高，以及出现目标的得分与每种类别的得分。经过predict_transform变换后的x的维度是(batch_size, grid_size*grid_size*num_anchors, 5+类别数量)\n","                x = predict_transform(x, inp_dim, anchors, num_classes, CUDA)\n","                 \n","                if not write:              #if no collector has been intialised. 因为一个空的tensor无法与一个有数据的tensor进行concatenate操作，\n","                    detections = x #所以detections的初始化在有预测值出来时才进行，\n","                    write = 1   #用write = 1标记，当后面的分数出来后，直接concatenate操作即可。\n","        \n","                else:  \n","                    '''\n","                    变换后x的维度是(batch_size, grid_size*grid_size*num_anchors, 5+类别数量)，这里是在维度1上进行concatenate，即按照\n","                    anchor数量的维度进行连接，对应教程part3中的Bounding Box attributes图的行进行连接。yolov3中有3个yolo层，所以\n","                    对于每个yolo层的输出先用predict_transform()变成每行为一个anchor对应的预测值的形式(不看batch_size这个维度，x剩下的\n","                    维度可以看成一个二维tensor)，这样3个yolo层的预测值按照每个方框对应的行的维度进行连接。得到了这张图处所有anchor的预测值，后面的NMS等操作可以一次完成\n","                    '''\n","                    detections = torch.cat((detections, x), 1)# 将在3个不同level的feature map上检测结果存储在 detections 里\n","        \n","            outputs[i] = x\n","\n","            \n","        def load_weights(self, weightfile):\n","          fp = open(weightfile, \"rb\") #r :　read as str/ rb: read as bytes\n","          #The first 5 values are header information   \n","          # 1. Major version number \n","          # 2. Minor Version Number\n","          # 3. Subversion number \n","          # 4,  5. Images seen by the network (during training)\n","          header = np.fromfile(fp, dtype = np.int32, count=5)\n","          self.header = torch.from_numpy(header)\n","          self.seen = self.header[3]\n","\n","          weights = np.fromfile(fp, dtype = np.float32)\n","\n","          ptr = 0\n","          for i in range(len(self.module_list)):\n","            module_type = self.blocks[i+1][\"type\"] #blocks[0]是net描述文字區，真正的module文字要從blocks[1]開始\n","              #If module_type is convolutional load weights\n","              #Otherwise ignore.\n","\n","            if module_type == \"convolutional\":\n","              model = self.module_list[i]\n","              try:\n","                batch_normalize = int(self.blocks[i+1][\"batch_normalize\"]) \n","                #有就 = 1\n","              except:\n","                batch_normalize = 0\n","\n","              conv = model[0]\n","\n","              if(batch_normalize):\n","                bn = model[1] #若有bn層，即在conv下一層\n","                num_bn_bias = bn.bias.numel() #numel:獲得矩陣元素個數。此處為獲得預設的權重參數個數\n","                \n","                #Load the weights\n","                bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_bias])\n","                ptr += num_bn_biases\n","\n","                bn_weights = torch.from_numpy(weights[ptr:ptr + num_bn_bias])\n","                ptr += num_bn_biases\n","\n","                bn_running_mean = torch.from_numpy(weights[ptr:ptr + num_bn_bias])\n","                ptr += num_bn_biases\n","\n","                bn_running_var = torch.from_numpy(weights[ptr:ptr + num_bn_bias])\n","                ptr += num_bn_biases\n","\n","\n","                #Cast the loaded weights into dims of model weights. \n","                bn_biases = bn_biases.view_as(bn.bias.data)\n","                bn_weights = bn_weights.view_as(bn.weight.data)\n","                bn_running_mean = bn_running_mean.view_as(bn.running_mean)\n","                bn_running_var = bn_running_var.view_as(bn.running_var)\n","\n","                #Copy the data to model 将从weights文件中得到的权重bn_biases复制到model中(bn.bias.data)\n","                bn.bias.data.copy_(bn_biases)\n","                bn.weights.data.copy_(bn_weights)\n","                bn.running_mean.copy_(bn_running_mean)\n","                bn.running_var.copy_(bn_running_var)\n","              \n","              else:\n","                #如果 batch_normalize 的检查结果不是 True，只需要加载卷积层的偏置项\n","                #Number of biases \n","                num_biases = conv.bias.numel()\n","\n","                conv_biases = torch.from_numpy(weights[ptr:ptr + num_biases])\n","                ptr = ptr + num_biases\n","\n","                conv_biases = conv_biases.view_as(conv.bias.data)\n","\n","                conv.bias.data.copy_(conv_biases)\n","              \n","              # 有bn:加載bn的bias、weights、running_mean & var\n","              # 沒bn:加載conv的bias\n","              # 然後都要加載conv的weights\n","\n","              num_weights = conv.weights.numel()\n","\n","              conv_weights = torch.from_numpy(weights[ptr:ptr+num_weights])\n","              ptr+=num_weights\n","\n","              conv_weights = conv_weights.view_as(conv.weights.data)\n","              conv.weights.data.copy_(conv_weights)          \n","\n","\n","\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C0C3_OSJuUHU"},"source":["Darknet:C語言的開源框架、用已記錄下來cfg檔中的參數來run\n","1. 用迴圈遍歷每個BLOCK，跑每個BLOCK的Layer並傳遞特徵x，然後放在outputs中(outputs：{1:x1, 2:x2, 3:x3})\n","2. layers 的記法為 num1, num2(optional)，其中若為負數，則表示往回幾個blocks，若為正數，表示跳到指定第幾個block\n","3. shortcut 為何要相加(ResNet的運用?)将输入的特征图，与输出特征图对应维度进行相加，即y = f(x) + x\n","4. route 為何要concat: concat操作源于DenseNet网络的设计思路，将特征图按照通道维度直接进行拼接，例如8*8*16的特征图与8*8*16的特征图拼接后生成8*8*32的特征图。\n","\n","Weights\n","1. 獲得原本模型預設的conv/bn參數數量k\n","2. 用k去取得weights檔裡對應的數字並轉成torch(from_numpy:numpy->torch)\n","3. 將獲得的weights數字們轉成模型需要的矩陣形狀(view_as)\n","4. 將其copy到模型的data裡\n","5. weights檔長怎樣?\n","6. bn.bias.data長怎樣"]},{"cell_type":"markdown","metadata":{"id":"hT3MtYh6E-ZW"},"source":["util.py。这个程序包含了predict_transform函数（Darknet类中的forward函数要用到），write_results函数使我们的输出满足 objectness 分数阈值和非极大值抑制（NMS），以得到「真实」检测结果。还有prep_image和letterbox_image等图片预处理函数等（前者用来将numpy数组转换成PyTorch需要的的输入格式，后者用来将图片按照纵横比进行缩放，将空白部分用(128,128,128)填充）。话不多说，直接看util.py的注释。"]},{"cell_type":"code","metadata":{"id":"S4M09Jv9-t5V"},"source":["def unique(tensor):#因为同一类别可能会有多个真实检测结果，所以我们使用unique函数来去除重复的元素，即一类只留下一个元素，达到获取任意给定图像中存在的类别的目的。\n","  tensor_np = tensor.cpu().numpy()\n","  unique_np = np.unique(tensor_np)#np.unique该函数是去除数组中的重复数字，并进行排序之后输出\n","  unique_tensor = torch.from_numpy(unique_np)\n","  # 复制数据\n","  tensor_res = tensor.new(unique_tensor.shape)# new(args, *kwargs) 构建[相同数据类型]的新Tensor\n","  tensor_res.copy_(unique_tensor)\n","  # why not tensor_res = unique_tensor 就好\n","  return tensor_res\n","\n","def bbox_iou(box1, box2):\n","      \"\"\"\n","    Returns the IoU of two bounding boxes \n","    \n","    \n","    \"\"\"\n","    #Get the coordinates of bounding boxes\n","    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,2], box1[:,3] \n","    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,2], box2[:,3] \n","\n","    #get the corrdinates of the intersection rectangle\n","    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n","    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n","    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n","    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n","    #Intersection area\n","    # Intersection area 这里没有对inter_area为负的情况进行判断，后面计算出来的IOU就可能是负的\n","    # clamp:避免沒有交集時變成負值\n","    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 +1, min=0) * torch.clamp(inter_rect_y2 - inter_rect_y1 +1 , min=0) \n","\n","    b1_area = (b1_x2 - b1_x1 +1 )*(b1_y2 - b1_y1 +1 )\n","    b2_area = (b2_x2 - b2_x1 +1 )*(b2_y2 - b2_y1 +1 )\n","\n","    iou = inter_area/(b1_area + b2_area - inter_area)\n","    return iou\n","\n","  def  predict_transform(prediction, inp_dim, anchors, num_classes, CUDA = True)\n","    \"\"\"\n","    在特征图上进行多尺度预测, 在GRID每个位置都有三个不同尺度的锚点.predict_transform()利用一个scale得到的feature map预测得到的每个anchor的属性(x,y,w,h,s,s_cls1,s_cls2...),其中x,y,w,h\n","    是在网络输入图片坐标系下的值,s是方框含有目标的置信度得分，s_cls1,s_cls_2等是方框所含目标对应每类的概率。输入的feature map(prediction变量) \n","    维度为(batch_size, num_anchors*bbox_attrs, grid_size, grid_size)，类似于一个batch彩色图片BxCxHxW存储方式。参数见predict_transform()里面的变量。\n","    并且将结果的维度变换成(batch_size, grid_size*grid_size*num_anchors, 5+类别数量)的tensor，同时得到每个方框在网络输入图片(416x416)坐标系下的(x,y,w,h)以及方框含有目标的得分以及每个类的得分。\n","    \"\"\"\n","    batch_size = prediction.size(0)\n","    # stride表示的是整个网络的步长，等于图像原始尺寸与yolo层输入的feature mapr尺寸相除，因为输入图像是正方形，所以用高相除即可\n","    stride = inp_dim//prediction.size(2)#416//13= 32  假設feature_map是13x13\n","\n","    # feature map每条边格子的数量，416//32 = 13\n","    grid_size = inp_dim//stride\n","\n","    bbox_attrs = 5 + num_classes #框的屬性包含5(x,y,w,h,置信度=1or0) + 各類別機率\n","\n","    num_anchors = len(anchors)\n","\n","    # 输入的prediction维度为(batch_size, num_anchors * bbox_attrs, grid_size, grid_size)，类似于一个batch彩色图片BxCxHxW\n","    # 存储方式，将它的维度变换成(batch_size, bbox_attrs*num_anchors, grid_size*grid_size)\n","    prediction = prediction.view(batch_size, bbox_attrs*num_anchors, grid_size*grid_size) #300x (5x85) x (16x16)\n","\n","    #contiguous：view只能用在contiguous的variable上。如果在view之前用了transpose, permute等，需要用contiguous()来返回一个contiguous copy。\n","    prediction = prediction.transpose(1,2).contiguous()\n","\n","    # 将prediction维度转换成(batch_size, grid_size*grid_size*num_anchors, bbox_attrs)。不看batch_size，\n","    # (grid_size*grid_size*num_anchors, bbox_attrs)相当于将所有anchor按行排列，即一行对应一个anchor属性，此时的属性仍然是feature map得到的值   \n","    prediction = prediction.view(batch_size, grid_size*grid_size*num_anchors, bbox_attrs ) \n","    \n","    # 锚点的维度与net块的height和width属性一致。这些属性描述了输入图像的维度，比feature map的规模大（二者之商即是步幅）。因此，我们必须使用stride分割锚点。变换后的anchors是相对于最终的feature map的尺寸\n","    anchors = [(a[0]/stride, a[1]/stride) for a in anchors]\n","\n","    #Sigmoid the tX, tY. and object confidencce.tx与ty为预测的坐标偏移值\n","    prediction[:,:,0] = torch.sigmoid(prediction[:,:,0])\n","    prediction[:,:,1] = torch.sigmoid(prediction[:,:,1])\n","    prediction[:,:,4] = torch.sigmoid(prediction[:,:,4])\n","\n","    #这里生成了每个格子的左上角坐标，生成的坐标为grid x grid的二维数组，a，b分别对应这个二维矩阵的x,y坐标的数组，a,b的维度与grid维度一样。每个grid cell的尺寸均为1，故grid范围是[0,12]（假如当前的特征图13*13）\n","    grid = np.arrange(grid_size)\n","    a,b = np.meshgrid(grid, grid) #(0,0 0,1...0,12, 1,0...1,12 ... 12,0...12,12)\n","    x_offset = torch.FloatTensor(a).view(-1,1)\n","    y_offset = torch.FloarTensor(b).view(-1,1)\n","\n","    if CUDA:\n","      x_offset = x_offset.cuda()\n","      y_offset = y_offset.cuda()\n","    \n","    #这里的x_y_offset对应的是最终的feature map中每个格子的左上角坐标，比如有13个格子，刚x_y_offset的坐标就对应为(0,0),(0,1)…(12,12) .view(-1, 2)将tensor变成两列，unsqueeze(0)在0维上添加了一维。\n","    x_y_offset = torch.cat((x_offset, y_offset), 1).repeat(1, num_anchors).view(-1,2).unsqueeze()\n","    prediction[:,:,:2] += x_y_offset#bx=sigmoid(tx)+cx,by=sigmoid(ty)+cy\n","    \n","    if CUDA:\n","      anchors = anchors.cuda()\n","\n","    # 这里的anchors本来是一个长度为6的list(三个anchors每个2个坐标)，然后在0维上(行)进行了grid_size*grid_size个复制，在1维(列)上\n","    # 一次复制(没有变化)，即对每个格子都得到三个anchor。Unsqueeze(0)的作用是在数组上添加一维，这里是在第0维上添加的。添加grid_size是为了之后的公式bw=pw×e^tw的tw。    \n","    anchors = anchors.repeat(grid_size*grid_size, 1).unsqueeze(0)\n","\n","    #对网络预测得到的矩形框的宽高的偏差值进行指数计算，然后乘以anchors里面对应的宽高(这里的anchors里面的宽高是对应最终的feature map尺寸grid_size)，\n","    # 得到目标的方框的宽高，这里得到的宽高是相对于在feature map的尺寸\n","    prediction[:,:,2:4] = torch.exp(prediction[:,:,2:4])*anchors #[(pw,ph),(pw,ph)...] * [(exp(tw),exp(th)),(exp(tw),exp(th))...] (對應相乘)\n","    \n","    # 这里得到每个anchor中每个类别的得分。将网络预测的每个得分用sigmoid()函数计算得到\n","    prediction[:,:,5: 5+ num_classes] = torch.sigmoid((prediction[:,:, 5+ num_classes]))\n","\n","    prediction[:,:,:4] *= stride\n","    #将相对于最终feature map的方框坐标和尺寸映射回输入网络图片(416x416)，即将方框的坐标乘以网络的stride即可\n","\n","    return prediction"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0EHINKI4KRXQ"},"source":["unique func\n","1. .cdu().numpy()\n","That’s because numpy doesn’t support CUDA, so there’s no way to make it use GPU memory without a copy to CPU first. Remember that .numpy() doesn’t do any copy, but returns an array that uses the same memory as the tensor.\n","2. clamp():限制輸出在min max間(小於min就輸出min，大於max就輸出max)\n","3. why IOU計算要+1\n","4. Contiguous跟記憶體存取有關(執行view記憶體需連續)\n","5. meshgrid 生成範圍內的每個網格樣本點(以code為例，即生成0,0 0,1...0,12, 1,0...1,12 ... 12,0...12,12)\n","6. 對f_map的每格都做計算，最後用置信度篩出最可能的\n","在prediction view的時候就把predictiont拆成batch_size x grid_size*grid_size*nums_anchors x bbox_attrs\n","中間那項即在每格都計算一個數值(長向量)"]},{"cell_type":"code","metadata":{"id":"TZMQ6TA13sIX","executionInfo":{"status":"ok","timestamp":1600930479655,"user_tz":-480,"elapsed":668,"user":{"displayName":"黃鈺倫 Intern","photoUrl":"","userId":"07415287677551779104"}},"outputId":"6160d2d0-fc1d-4591-eb18-b6573ef3c82f","colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["import torch\n","anchors = torch.tensor([(13,30),(30,36),(36,40)])\n","grid_size = 13\n","anchors = anchors.repeat(grid_size*grid_size, 1).unsqueeze(0)\n","print(anchors[:,0:10,0:2])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[[13, 30],\n","         [30, 36],\n","         [36, 40],\n","         [13, 30],\n","         [30, 36],\n","         [36, 40],\n","         [13, 30],\n","         [30, 36],\n","         [36, 40],\n","         [13, 30]]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sDVj7SR7OjaF"},"source":["    '''\n","    必须使我们的输出满足 objectness 分数阈值和非极大值抑制（NMS），以得到后文所提到的「真实」检测结果。要做到这一点就要用 write_results函数。\n","    函数的输入为预测结果、置信度（objectness 分数阈值）、num_classes（我们这里是 80）和 nms_conf（NMS IoU 阈值）。\n","    write_results()首先将网络输出方框属性(x,y,w,h)转换为在网络输入图片(416x416)坐标系中，方框左上角与右下角坐标(x1,y1,x2,y2)，以方便NMS操作。\n","    然后将方框含有目标得分低于阈值的方框去掉，提取得分最高的那个类的得分max_conf，同时返回这个类对应的序号max_conf_score,\n","    然后进行NMS操作。最终每个方框的属性为(ind,x1,y1,x2,y2,s,s_cls,index_cls)，ind 是这个方框所属图片在这个batch中的序号，\n","    x1,y1是在网络输入图片(416x416)坐标系中，方框左上角的坐标；x2,y2是在网络输入图片(416x416)坐标系中，方框右下角的坐标。\n","    s是这个方框含有目标的得分,s_cls是这个方框中所含目标最有可能的类别的概率得分，index_cls是s_cls对应的这个类别所对应的序号.\n","    '''\n","def write_results(prediction, confidence, num_classes, nms_conf=0.4):\n","  # confidence: 输入的预测shape=(1,10647, 85)。conf_mask: shape=(1,10647) => 增加一维度之后 (1, 10647, 1) \n","  #85類只抓一類出來\n","  conf_mask = (prediction[:,:,4]>confidence).float().unsqueeze(2)\n","  prediction = prediction*conf_mask\n","  #與True/False相乘 =>  \n","  # 小于置信度的条目值全为0, 剩下部分不变。conf_mask中含有目标的得分小于confidence的方框所对应的含有目标的得分为0，\n","  #根据numpy的广播原理，它会扩展成与prediction维度一样的tensor，所以含有目标的得分小于confidence的方框所有的属性都会变为0，故如果没有检测任何有效目标,则返回值为0\n","\n","  '''\n","  保留预测结果中置信度大于阈值的bbox\n","  下面开始为nms准备\n","  '''\n","\n","  # prediction的前五个数据分别表示 (Cx, Cy, w, h, score)，这里创建一个新的数组，大小与predicton的大小相同   \n","  box_corner = prediction.new(prediction.shape)\n","  '''\n","  我们可以将我们的框的 (中心 x, 中心 y, 高度, 宽度) 属性转换成 (左上角 x, 左上角 y, 右下角 x, 右下角 y)\n","  这样做用每个框的两个对角坐标能更轻松地计算两个框的 IoU\n","  '''\n","  box_corner[:,:,0] = (prediction[:,:,0] - prediction[:,:,2]/2)# x1 = Cx - w/2\n","  box_corner[:,:,1] = (prediction[:,:,1] - prediction[:,:,3]/2)# y1 = Cy - h/2\n","  box_corner[:,:,2] = (prediction[:,:,0] + prediction[:,:,2]/2)# x2 = Cx + w/2 \n","  box_corner[:,:,3] = (prediction[:,:,1] + prediction[:,:,3]/2)# y2 = Cy + h/2\n","  prediction[:,:,:4] = box_corner[:,:,:4]# 计算后的新坐标复制回去\n","\n","  batch_size = prediction.size(0)\n","  # 第0个维度是batch_size\n","  # output = prediction.new(1, prediction.size(2)+1) # shape=(1,85+1)\n","  write = False# 拼接结果到output中最后返回\n","  #对每一张图片得分的预测值进行NMS操作，因为每张图片的目标数量不一样，所以有效得分的方框的数量不一样，没法将几张图片同时处理，因此一次只能完成一张图的置信度阈值的设置和NMS,不能将所涉及的操作向量化.\n","  #所以必须在预测的第一个维度上（batch数量）上遍历每张图片，将得分低于一定分数的去掉，对剩下的方框进行进行NMS\n","\n","  for ind in range(batch_size):\n","    image_pred = prediction[ind]# 选择此batch中第ind个图像的预测结果,image_pred对应一张图片中所有方框的坐标(x1,y1,x2,y2)以及得分，是一个二维tensor 维度为10647x85\n","\n","    \n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-VeGs8PrbRjf"},"source":["1. confidence是一個值?"]},{"cell_type":"code","metadata":{"id":"kCd3sqWPZ7cf","executionInfo":{"status":"ok","timestamp":1601107843707,"user_tz":-480,"elapsed":939,"user":{"displayName":"黃鈺倫 Intern","photoUrl":"","userId":"07415287677551779104"}},"outputId":"5e70af2d-71c8-410e-eede-679426f2b2cb","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["10647/3/13/13"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["21.0"]},"metadata":{"tags":[]},"execution_count":4}]}]}