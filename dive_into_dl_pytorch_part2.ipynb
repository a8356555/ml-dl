{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dive_into_dl_pytorch_part2.ipynb","provenance":[],"authorship_tag":"ABX9TyOPD0fMZo4vLLrLCzdAkfT1"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X0qUS-4xK4kP","executionInfo":{"elapsed":8409,"status":"ok","timestamp":1607008570809,"user":{"displayName":"黃鈺倫 Intern","photoUrl":"","userId":"07415287677551779104"},"user_tz":-480},"outputId":"8d8cf6f0-33d0-4fcf-d341-c79c4c1ecca5"},"source":["!git clone https://github.com/d2l-ai/d2l-zh.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'd2l-zh'...\n","remote: Enumerating objects: 94, done.\u001b[K\n","remote: Counting objects: 100% (94/94), done.\u001b[K\n","remote: Compressing objects: 100% (89/89), done.\u001b[K\n","remote: Total 17279 (delta 26), reused 24 (delta 5), pack-reused 17185\u001b[K\n","Receiving objects: 100% (17279/17279), 185.22 MiB | 36.93 MiB/s, done.\n","Resolving deltas: 100% (12059/12059), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uZQruHjXLLAw"},"source":["import torch\n","import numpy as np\n","\n","%matplotlib inline\n","from IPython import display\n","import matplotlib.pyplot as plt\n","import random\n","\n","import torch.utils.data as Data\n","\n","import torch.nn as nn\n","from torch.nn import init\n","\n","import torch.optim as optim\n","from collections import OrderedDict\n","\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","import time\n","import sys\n","sys.path.append(\"..\") # 为了导入上层目录的d2lzh_pytorch\n","# import d2lzh_pytorch as d2l\n","\n","import torchtext.vocab as Vocab\n","import os\n","from tqdm import tqdm\n","import tarfile\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","DATA_ROOT = \"/S1/CSCL/tangss/Datasets\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IVgsHRVDLPhm"},"source":["import zipfile\n","import os\n","data_root = '/content/d2l-zh/data'\n","fname = os.path.join(data_root, 'ptb.zip')\n","with zipfile.ZipFile(fname, 'r') as f:\n","  f.extractall(data_root)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"A1AIjbC3LRiI"},"source":["import collections\n","assert 'ptb.train.txt' in os.listdir(data_root + \"/ptb\")\n","import time, math\n","with open('/content/d2l-zh/data/ptb/ptb.train.txt', 'r') as f:\n","  lines = f.readlines() #每個elem為每行\n","  raw_dataset = [st.split() for st in lines] #每行中的每個elem為每個字\n","counter = collections.Counter([tk for st in raw_dataset for tk in st]) #return dict{words:count_nums}\n","counter = dict(filter(lambda x:x[1]>=5,counter.items())) \n","\n","idx_to_token = [tk for tk, _ in counter.items()] #idx => idx_to_token[idx] return token\n","token_to_idx = {tk:idx for idx, tk in enumerate(idx_to_token)} #token => token_to_idx[tk] return idx\n","dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx] for st in raw_dataset]\n","def discard(idx): #當任意機率<丟棄機率，就為True(準備丟棄)\n","  return random.uniform(0, 1) < 1 - math.sqrt(1e-4/counter[idx_to_token[idx]]*sum([len(st) for st in dataset]))\n","\n","subsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]"],"execution_count":null,"outputs":[]}]}